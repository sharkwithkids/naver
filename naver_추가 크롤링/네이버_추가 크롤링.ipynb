{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6dfb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì „ì²´ ì›¹íˆ° ìˆ˜: 2468\n",
      "\n",
      "ğŸ”„ [1/25] í¬ë¡¤ë§ ì‹œì‘: index 0~99\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_detail_data(title_id):\n",
    "    base_url = f\"https://comic.naver.com/webtoon/detail?titleId={title_id}&no=\"\n",
    "    episodes = range(1, 6)\n",
    "    data = []\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('window-size=1920x1080')\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        for no in episodes:\n",
    "            url = f\"{base_url}{no}&week=finish\"\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            try:\n",
    "                like = int(soup.select_one(\"em.u_cnt._count\").text.strip().replace(\",\", \"\"))\n",
    "            except:\n",
    "                like = None\n",
    "\n",
    "            try:\n",
    "                star = float(soup.select_one(\"span.UserAction__score--sP1ha\").text.strip())\n",
    "            except:\n",
    "                star = None\n",
    "\n",
    "            try:\n",
    "                tag = soup.select_one(\"button.UserAction__star--bi4ea span.UserAction__count--jk3vo\")\n",
    "                star_cnt = int(tag.text.strip().replace(\",\", \"\").replace(\"ì°¸ì—¬\", \"\").strip()) if tag else None\n",
    "            except:\n",
    "                star_cnt = None\n",
    "\n",
    "            try:\n",
    "                comment_cnt = int(soup.select_one(\"span.u_cbox_count\").text.strip().replace(\",\", \"\"))\n",
    "            except:\n",
    "                comment_cnt = None\n",
    "\n",
    "            try:\n",
    "                best_likes = [\n",
    "                    int(tag.text.strip().replace(\",\", \"\"))\n",
    "                    for tag in soup.select(\"em.u_cbox_cnt_recomm\")[:15]\n",
    "                ]\n",
    "            except:\n",
    "                best_likes = []\n",
    "\n",
    "            data.append({\n",
    "                \"title_id\": title_id,\n",
    "                \"episode\": no,\n",
    "                \"like_count\": like,\n",
    "                \"star\": star,\n",
    "                \"star_participants\": star_cnt,\n",
    "                \"comment_count\": comment_cnt,\n",
    "                \"best_comment_likes\": best_likes\n",
    "            })\n",
    "    except:\n",
    "        data = [{\n",
    "            \"title_id\": title_id,\n",
    "            \"episode\": \"ì„±ì¸ë¬¼ ì°¨ë‹¨ë¨\",\n",
    "            \"like_count\": None,\n",
    "            \"star\": None,\n",
    "            \"star_participants\": None,\n",
    "            \"comment_count\": None,\n",
    "            \"best_comment_likes\": []\n",
    "        }]\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return data\n",
    "\n",
    "# ì‹¤í–‰ ì‹œì‘\n",
    "title_df = pd.read_csv(\"naver_listly_updated.csv\")\n",
    "id_title_list = title_df[['ID', 'ì œëª©']].astype({'ID': str}).values.tolist()\n",
    "num_id_list_total = len(id_title_list)\n",
    "num_id_csv_files = num_id_list_total // 100 + 1\n",
    "\n",
    "raw_all = []\n",
    "summary_all = []\n",
    "\n",
    "print(\"ğŸš€ ì „ì²´ ì›¹íˆ° ìˆ˜:\", num_id_list_total)\n",
    "\n",
    "for idx in range(num_id_csv_files):\n",
    "    start, end = idx * 100, min((idx + 1) * 100, num_id_list_total)\n",
    "    id_list = id_title_list[start:end]\n",
    "\n",
    "    print(f\"\\nğŸ”„ [{idx+1}/{num_id_csv_files}] í¬ë¡¤ë§ ì‹œì‘: index {start}~{end-1}\")\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    raw_rows = []\n",
    "    summary_rows = []\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    for title_id, title in id_list:\n",
    "        rows = crawl_detail_data(title_id)\n",
    "        raw_rows.extend(rows)\n",
    "\n",
    "        if rows and isinstance(rows[0][\"episode\"], int):\n",
    "            success_count += 1\n",
    "            df = pd.DataFrame(rows)\n",
    "            total_likes = sum([sum(lst) for lst in df[\"best_comment_likes\"] if lst])\n",
    "            total_counts = sum([len(lst) for lst in df[\"best_comment_likes\"] if lst])\n",
    "            avg_best_like = round(total_likes / total_counts, 2) if total_counts > 0 else 0\n",
    "\n",
    "            summary = {\n",
    "                \"title_id\": title_id,\n",
    "                \"title\": title,\n",
    "                \"avg_like_count\": round(df[\"like_count\"].mean(), 2),\n",
    "                \"avg_star\": round(df[\"star\"].mean(), 2),\n",
    "                \"avg_participants\": round(df[\"star_participants\"].mean(), 2),\n",
    "                \"avg_comment_count\": round(df[\"comment_count\"].mean(), 2),\n",
    "                \"avg_best_comment_like\": avg_best_like\n",
    "            }\n",
    "        else:\n",
    "            fail_count += 1\n",
    "            summary = {\n",
    "                \"title_id\": title_id,\n",
    "                \"title\": title,\n",
    "                \"avg_like_count\": \"ì„±ì¸ë¬¼ ì°¨ë‹¨ë¨\",\n",
    "                \"avg_star\": \"ì„±ì¸ë¬¼ ì°¨ë‹¨ë¨\",\n",
    "                \"avg_participants\": \"ì„±ì¸ë¬¼ ì°¨ë‹¨ë¨\",\n",
    "                \"avg_comment_count\": \"ì„±ì¸ë¬¼ ì°¨ë‹¨ë¨\",\n",
    "                \"avg_best_comment_like\": \"ì„±ì¸ë¬¼ ì°¨ë‹¨ë¨\"\n",
    "            }\n",
    "        summary_rows.append(summary)\n",
    "\n",
    "    raw_df = pd.DataFrame(raw_rows)\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    raw_df.to_csv(f\"webtoon_detail_raw_{idx}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    summary_df.to_csv(f\"webtoon_detail_summary_{idx}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    raw_all.append(raw_df)\n",
    "    summary_all.append(summary_df)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: webtoon_detail_raw_{idx}.csv, webtoon_detail_summary_{idx}.csv\")\n",
    "    print(f\"â±ï¸ ì†Œìš” ì‹œê°„: {t1 - t0:.2f}ì´ˆ | ìˆ˜ì§‘ ì„±ê³µ: {success_count}, ì°¨ë‹¨: {fail_count}\")\n",
    "\n",
    "# ì „ì²´ í†µí•© ì €ì¥\n",
    "pd.concat(raw_all).to_csv(\"webtoon_detail_raw_all.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "pd.concat(summary_all).to_csv(\"webtoon_detail_summary_all.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nğŸ‰ ì „ì²´ í†µí•© íŒŒì¼ ì €ì¥ ì™„ë£Œ: webtoon_detail_raw_all.csv, webtoon_detail_summary_all.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
